# Домашнее задание к занятию «Микросервисы: масштабирование»

## ` Дмитрий Климов `

## Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры. Вам как DevOps-специалисту             необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.

## Задача 1: Кластеризация
Предложите решение для обеспечения развёртывания, запуска и управления приложениями. Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:

  * поддержка контейнеров;
  * обеспечивать обнаружение сервисов и маршрутизацию запросов;
  * обеспечивать возможность горизонтального масштабирования;
  * обеспечивать возможность автоматического масштабирования;
  * обеспечивать явное разделение ресурсов, доступных извне и внутри системы;
  * обеспечивать возможность конфигурировать приложения с помощью переменных среды, в том числе с возможностью безопасного хранения      чувствительных данных таких как пароли, ключи доступа, ключи шифрования и т. п.

### Обоснуйте свой выбор.

## Ответ:

### ` В качестве оптимального решения для крупной компании я предлагаю использование платформы оркестрации **Kubernetes (K8s)**. На сегодняшний день это промышленный стандарт, обладающий самой развитой экосистемой и полностью закрывающий потребности микросервисной архитектуры. `

### Предлагаемый технологический стек:
*   **Оркестратор:** Kubernetes.
*   **Среда выполнения:** containerd.
*   **Маршрутизация:** Ingress Nginx Controller.
*   **Управление секретами:** HashiCorp Vault (интегрированный с K8s).
*   **Автоматизация:** Helm (для управления пакетами) и ArgoCD (для реализации GitOps).

---

## Соответствие решения требованиям:

### 1. Поддержка контейнеров
Kubernetes является нативным решением для управления контейнерами. Он работает со средами выполнения, поддерживающими стандарт CRI (Container Runtime Interface), такими как **containerd** или **CRI-O**. Это позволяет упаковывать микросервисы в Docker-образы и обеспечивать их идентичную работу на всех этапах (Dev/Stage/Prod).

### 2. Обнаружение сервисов (Service Discovery) и маршрутизация запросов
*   **Обнаружение:** Внутри кластера используется встроенный сервис **CoreDNS**. Каждый сервис получает внутреннее DNS-имя, что позволяет микросервисам обращаться друг к другу без знания IP-адресов.
*   **Маршрутизация:** Для управления входящим трафиком используется **Ingress Controller**. Он обеспечивает гибкую маршрутизацию на основе путей (path) или доменных имен (host), а также терминирует SSL-сертификаты.

### 3. Горизонтальное масштабирование
Реализуется через контроллеры **Deployment** или **StatefulSet**. Путем изменения параметра `replicas` (вручную или через CI/CD) можно мгновенно увеличить количество экземпляров приложения. Kubernetes автоматически распределит нагрузку между всеми запущенными подами через объект **Service**.

### 4. Автоматическое масштабирование
Для этого используются два механизма:
*   **Horizontal Pod Autoscaler (HPA):** автоматически меняет количество подов в зависимости от нагрузки (CPU, RAM или кастомные метрики из Prometheus).
*   **Cluster Autoscaler:** если для новых подов не хватает физических ресурсов, он автоматически добавляет новые узлы (worker nodes) в кластер (при работе в облаке).

### 5. Разделение ресурсов (внешние и внутренние)
*   **Логическое разделение:** Использование **Namespaces** для изоляции окружений или бизнес-юнитов.
*   **Сетевое разделение:** С помощью **Network Policies** (аналог файрвола внутри кластера) можно запретить прямой доступ к базам данных или внутренним API извне, разрешая трафик только от определенных микросервисов.
*   **Доступ:** Внешний доступ предоставляется только через **Ingress**, в то время как внутренние сервисы имеют тип `ClusterIP` (доступны только внутри кластера).

### 6. Конфигурация и безопасное хранение данных
*   **ConfigMaps:** используются для передачи обычных настроек через переменные окружения.
*   **Secrets + HashiCorp Vault:** Для хранения чувствительных данных (пароли, ключи) рекомендуется использовать HashiCorp Vault. С помощью *External Secrets Operator* или *Vault Agent Injector* секреты безопасно доставляются в поды в виде переменных окружения или файлов, не сохраняясь в открытом виде в репозитории кода.

---

## Обоснование выбора

1.  **Масштабируемость и гибкость:** Kubernetes позволяет управлять тысячами контейнеров и автоматически подстраиваться под объемы трафика.
2.  **Отсутствие привязки к вендору (Vendor Lock-in):** Решение можно развернуть как в собственном дата-центре (on-premise), так и в любом облаке (Yandex Cloud, AWS, GCP).
3.  **Самовосстановление (Self-healing):** Если микросервис зависнет или упадёт, Kubernetes сам перезапустит его или перенесёт на другой исправный узел.
4.  **Развитое сообщество:** Огромное количество готовых решений (Helm-чарты, операторы) значительно сокращает время на настройку инфраструктуры и поиск специалистов.

## Задача 2: Распределённый кеш * (необязательная)

### Разработчикам вашей компании понадобился распределённый кеш для организации хранения временной информации по сессиям              пользователей. Вам необходимо построить Redis Cluster, состоящий из трёх шард с тремя репликами.

### ` Схема: `

<img width="960" height="720" alt="image" src="https://github.com/user-attachments/assets/c769783f-439d-4f22-b8ee-d7e21423c346" />

## Ответ:

Для организации распределённого хранилища сессий пользователей был развёрнут **Redis Cluster**, состоящий из 3 шард (мастер-узлов) и 3 реплик. Архитектура построена на базе трёх виртуальных машин для обеспечения отказоустойчивости.

### 1. Схема распределения узлов
Для реализации отказоустойчивости на каждой из 3 виртуальных машин запущено по 2 инстанса Redis (один мастер и одна реплика другого мастера).

*   **VM1 (192.168.0.116):** Master 1 (6379), Replica 3 (6380)
*   **VM2 (192.168.0.117):** Master 2 (6379), Replica 1 (6380)
*   **VM3 (192.168.0.118):** Master 3 (6379), Replica 2 (6380)

### 2. Конфигурация узлов
Каждый из 6 инстансов Redis настроен со следующими ключевыми параметрами в `redis.conf`:
*   `port`: 6379 / 6380
*   `cluster-enabled yes`: активация режима кластера.
*   `cluster-config-file nodes.conf`: файл для хранения состояния кластера.
*   `bind 0.0.0.0`: разрешение подключений со всех сетевых интерфейсов.
*   `cluster-node-timeout 5000`: время ожидания отклика узла.
*   `appendonly yes`: режим сохранения данных на диск.

### 3. Создание кластера
Инициализация кластера выполнена командой (с распределением 1 реплика на 1 мастер):
```bash
redis-cli --cluster create \
192.168.0.116:6379 192.168.0.117:6379 192.168.0.118:6379 \
192.168.0.117:6380 192.168.0.118:6380 192.168.0.116:6380 \
--cluster-replicas 1
```
<img width="1920" height="1080" alt="Снимок экрана (2580)" src="https://github.com/user-attachments/assets/e73ea3db-0cea-4d00-890e-ae7b9ba1d9fe" />

